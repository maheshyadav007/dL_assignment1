2021-03-10 18:35:20,111 INFO    Thread-20 :3616 [wandb_setup.py:_flush():69] setting env: {'entity': '-my', 'project': 'dl_assignment1', 'root_dir': 'D:\\IITM\\deepLearning\\Assignments\\assignment1', 'sweep_id': '00phcm50', 'run_id': 'ezc0wpid', 'sweep_param_path': 'D:\\IITM\\deepLearning\\Assignments\\assignment1\\wandb\\sweep-00phcm50\\config-ezc0wpid.yaml'}
2021-03-10 18:35:20,111 INFO    Thread-20 :3616 [wandb_setup.py:_flush():69] setting login settings: {}
2021-03-10 18:35:20,113 INFO    Thread-20 :3616 [wandb_init.py:_log_setup():319] Logging user logs to D:\IITM\deepLearning\Assignments\assignment1\wandb\run-20210310_183520-ezc0wpid\logs\debug.log
2021-03-10 18:35:20,114 INFO    Thread-20 :3616 [wandb_init.py:_log_setup():320] Logging internal logs to D:\IITM\deepLearning\Assignments\assignment1\wandb\run-20210310_183520-ezc0wpid\logs\debug-internal.log
2021-03-10 18:35:20,115 INFO    Thread-20 :3616 [wandb_init.py:init():352] calling init triggers
2021-03-10 18:35:20,115 INFO    Thread-20 :3616 [wandb_init.py:init():359] wandb.init called with sweep_config: {'alpha': 0, 'batch_size': 16, 'epochs': 10, 'hidden_Layer_AF': 'sigmoid', 'initializer': 'xavier', 'learning_rate': 0.001, 'loss_func': 'crossentropy', 'n_hidden_layers': 4, 'optimizer': 'nadam', 'size_hidden_layers': 64}
config: {'batch_size': 64, 'learning_rate': 0.001, 'epochs': 10, 'n_hidden_layers': 4, 'size_hidden_layers': 64, 'optimizer': 'adam'}
2021-03-10 18:35:20,116 INFO    Thread-20 :3616 [wandb_init.py:init():401] starting backend
2021-03-10 18:35:20,117 INFO    Thread-20 :3616 [backend.py:_multiprocessing_setup():71] multiprocessing start_methods=spawn, using: spawn
2021-03-10 18:35:20,130 INFO    Thread-20 :3616 [backend.py:ensure_launched():123] starting backend process...
2021-03-10 18:35:21,398 INFO    Thread-20 :3616 [backend.py:ensure_launched():128] started backend process with pid: 12320
2021-03-10 18:35:21,403 INFO    Thread-20 :3616 [wandb_init.py:init():406] backend started and connected
2021-03-10 18:35:21,416 INFO    Thread-20 :3616 [wandb_run.py:_config_callback():669] config_cb None None {'alpha': 0, 'batch_size': 16, 'epochs': 10, 'hidden_Layer_AF': 'sigmoid', 'initializer': 'xavier', 'learning_rate': 0.001, 'loss_func': 'crossentropy', 'n_hidden_layers': 4, 'optimizer': 'nadam', 'size_hidden_layers': 64}
2021-03-10 18:35:21,419 INFO    Thread-20 :3616 [wandb_init.py:init():446] updated telemetry
2021-03-10 18:35:21,425 INFO    Thread-20 :3616 [wandb_init.py:init():465] communicating current version
2021-03-10 18:35:24,696 INFO    Thread-20 :3616 [wandb_init.py:init():470] got version response upgrade_message: "wandb version 0.10.22 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"

2021-03-10 18:35:24,697 INFO    Thread-20 :3616 [wandb_init.py:init():478] communicating run to backend with 30 second timeout
2021-03-10 18:35:25,365 INFO    Thread-20 :3616 [wandb_init.py:init():503] starting run threads in backend
2021-03-10 18:35:30,418 INFO    Thread-20 :3616 [wandb_run.py:_console_start():1422] atexit reg
2021-03-10 18:35:30,423 INFO    Thread-20 :3616 [wandb_run.py:_redirect():1285] redirect: SettingsConsole.WRAP
2021-03-10 18:35:30,424 INFO    Thread-20 :3616 [wandb_run.py:_redirect():1320] Wrapping output streams.
2021-03-10 18:35:30,424 INFO    Thread-20 :3616 [wandb_run.py:_redirect():1336] Redirects installed.
2021-03-10 18:35:30,425 INFO    Thread-20 :3616 [wandb_init.py:init():527] run started, returning control to user process
2021-03-10 18:35:30,501 INFO    Thread-20 :3616 [wandb_setup.py:_flush():69] setting env: {'entity': '-my', 'project': 'dl_assignment1', 'root_dir': 'D:\\IITM\\deepLearning\\Assignments\\assignment1', 'sweep_id': '00phcm50', 'run_id': 'ezc0wpid', 'sweep_param_path': 'D:\\IITM\\deepLearning\\Assignments\\assignment1\\wandb\\sweep-00phcm50\\config-ezc0wpid.yaml'}
2021-03-10 18:35:30,501 INFO    Thread-20 :3616 [wandb_setup.py:_flush():69] setting login settings: {}
2021-03-10 18:35:30,502 INFO    Thread-20 :3616 [wandb_init.py:_log_setup():319] Logging user logs to D:\IITM\deepLearning\Assignments\assignment1\wandb\run-20210310_183530-ezc0wpid\logs\debug.log
2021-03-10 18:35:30,503 INFO    Thread-20 :3616 [wandb_init.py:_log_setup():320] Logging internal logs to D:\IITM\deepLearning\Assignments\assignment1\wandb\run-20210310_183530-ezc0wpid\logs\debug-internal.log
2021-03-10 18:35:30,505 INFO    Thread-20 :3616 [wandb_init.py:init():352] calling init triggers
2021-03-10 18:35:30,505 INFO    Thread-20 :3616 [wandb_init.py:init():359] wandb.init called with sweep_config: {'alpha': 0, 'batch_size': 16, 'epochs': 10, 'hidden_Layer_AF': 'sigmoid', 'initializer': 'xavier', 'learning_rate': 0.001, 'loss_func': 'crossentropy', 'n_hidden_layers': 4, 'optimizer': 'nadam', 'size_hidden_layers': 64}
config: {}
2021-03-10 18:35:30,506 INFO    Thread-20 :3616 [wandb_init.py:init():398] wandb.init() called when a run is still active
2021-03-10 18:37:14,636 ERROR   Thread-13 :3616 [internal_api.py:execute():126] 429 response executing GraphQL.
2021-03-10 18:37:14,636 ERROR   Thread-13 :3616 [internal_api.py:execute():127] {"error":"rate limit exceeded"}

2021-03-10 18:37:59,807 ERROR   Thread-13 :3616 [internal_api.py:execute():126] 429 response executing GraphQL.
2021-03-10 18:37:59,807 ERROR   Thread-13 :3616 [internal_api.py:execute():127] {"error":"rate limit exceeded"}

2021-03-10 18:38:44,830 ERROR   Thread-13 :3616 [internal_api.py:execute():126] 429 response executing GraphQL.
2021-03-10 18:38:44,831 ERROR   Thread-13 :3616 [internal_api.py:execute():127] {"error":"rate limit exceeded"}

2021-03-10 18:41:57,540 INFO    Thread-20 :3616 [wandb_run.py:finish():1078] finishing run -my/dl_assignment1/ezc0wpid
